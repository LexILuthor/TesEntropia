\chapter{Informazione ed Entropia per variabili casuali discrete}
\label{cha:intro}
\vspace{15pt}




\section{Informazione}
\label{sec:informazione}
\vspace{10pt}

Fondamentali in questa tesi saranno i concetti di Informazione ed entropia. Bisogna anzitutto specificare che in Probabilità il significato di Informazione ha un connotato diverso da quello della lingua parlata. Consideriamo ad esempio le seguenti frasi: 
\begin{enumerate}
\item[i.] Quando vado in palestra mi alleno
\item[ii.] Il vincitore delle prossime elezioni sarà Claudio Baglioni
\item[iii.] QUER W LKS E W
\end{enumerate}

Istintivamente diremo che la frase contente maggior informazione è $(ii)$ in quanto contiene un'informazione totalmente inaspettata e nuova, seguita poi da $(i)$ ed in fine $(iii)$ la quale non avendo significato non conterrà nessuna informazione.\\ Questa scala però tiene conto sia del significato della frase sia della quantità di \textit{sorpresa} che porta. In questo senso $(iii)$ non ha significato, ma porta \textit{sorpresa}, mentre $(ii)$ contiene sia significato che sorpresa.\\ Nel mondo della matematica si è visto che il concetto di \textit{significato} è difficile da esprimere e si è dunque preferito puntare sul concetto di \textit{sorpresa} per esprimere il significato d'\textit{informazione}.\\
Per definire in maniera rigorosa il concetto di \textbf{informazione} poniamoci in uno spazio di probabilità \spacep.\\
Dati due eventi $E_1,E_2 \in \mathcal{F}$ vogliamo che la nostra funzione d'informazione $I$ soddisfi alcuni criteri:

\begin{enumerate} 
\item $I(E)\geq 0$ per ogni $E\in \mathcal{F}$
\item se \p$(E_1)\leq $ \p$(E_2)$ allora $I(E_1)\geq I(E_2)$ 
\item se $E_1,E_2$ sono indipendenti allora $I(E_1\cup E_2)=I(E_1)+ I(E_2)$
\end{enumerate} 
Per soddisfare queste richieste viene naturalmente in mente la funzione $\log$, infatti:
\begin{defi}
In uno spazio di probabilità \spacep definiamo la funzione \textbf{informazione} $I: \mathcal{F}\to \mathbb{R}^+$ come:
\begin{equation}
I(E)=-\log_a(\mathbb{P} (E)).
\end{equation}
dove $a$ è una costante positiva (in alcuni testi la funzione viene moltiplicata per $K$,  ma tale costante è inutile dato che già scegliere la base coincide col moltiplicare per una costante, infatti: $\log_a(x)=\frac{\log_b(y)}{\log_b(a)} \bigg)$.
\end{defi}

Si verifica facilmente che la funzione $I$ così definita rispetta le proprietà preposte. L'unico intoppo nasce per un evento $E$ tale che \p $(E)=0$ in questo caso $I(E)=\infty$, questa occorrenza può essere interpretata come l'incapacità di ottenere informazioni da un evento impossibile. La funzione \textit{Informazione} possiede inoltre la proprietà di essere nulla qualora la probabilità di un evento sia $1$ cioè se un evento è certo, la sua realizzazione non ci fornirà alcuna informazione.\\
Essendo questa funzione spesso associata a codici è comodo scegliere $2$ come base del logaritmo, in questo modo supponendo di avere una variabile casuale $X$ con distribuzione di Bernoulli a parametro $p=\frac{1}{2}$ (il nostro messaggio sarà definito da un codice binario ($\{ 0, 1 \}$) abbiamo che 
\begin{equation}
I(X=0)=I(X=1)=-\log_2 { \bigg(\frac{1}{2} \bigg )} =1
\end{equation}
Per questo d'ora in avanti, salvo diversa indicazione, con $\log$ si intenderà $\log_2$.
\vspace{15pt}

\section{Entropia}
\label{sec:Entropia}
\vspace{10pt}

Il secondo concetto fondamentale trattato in questa tesi è quello di \textit{entropia}.\\
Data una variabile casuale discreta $X$ a valori $\{ x_1...x_n \}$ e con legge di probabilità $\{p_1...p_n \}$ ($p_i:=\mathbb{P}(X=x_i)$) non possiamo conoscere a priori il valore che assumerà $X$ e di conseguenza non possiamo sapere quanta informazione verrà inviata. Definiamo per questo l'\textit{entropia}.
\begin{defi}
Si dice \textbf{entropia} di una variabile casuale discreta $X$ il valore
\begin{equation}
H(X):=\mathbb{E}(I(X))=-\sum_{j=1}^np_j\Phi(p_j)
\end{equation}
dove
$$
\Phi(p):=
\begin{cases}
\log_2 {(p)} \ se \ p \neq 0 \\
0 \ se \  p=0
\end{cases}
$$
\end{defi}
Per capire il senso di questa definizione si immagini di voler scommettere con una moneta modificata come segue:
\begin{enumerate}
\item esce testa con probabilità $p_1=0.95$
\item esce testa con probabilità $p_2=0.6$
\item esce testa con probabilità $p_3=0.5$
\end{enumerate} 
 usando la definizione di entropia otteniamo:
 
\begin{enumerate}
\item$H_1(p_1)=0.286$
\item$H_2(p_2)=0.971$
\item$H_3(p_3)=1$
\end{enumerate}
Ovviamente nel primo caso la probabilità di predire il risultato corretto è molto alta dato che la moneta è pesantemente modificata e infatti il sistema avrà una bassa entropia, nel secondo caso l'entropia aumenta, infine nel terzo l'indecisione sarà massima e l'entropia di conseguenza.\\
Per convincersi di quanto detto in maniera più matematica, si ha il seguente teorema:
\begin{teo} \label{teo:6.2}
Sia $X$ una variabile casuale discreta, allora vale:
\begin{enumerate}
\item $H(X)\geq 0$ e $H(X)= 0$ se e solo se esiste un valore di $X$, $x_1$ t.c. \p$(x_1)=1$
\item $H(X)\leq \log{(n)}$ e l'uguaglianza varrà solo quando $X$ ha distribuzione uniforme
\end{enumerate}
\end{teo}
\begin{proof} \leavevmode 
\begin{enumerate}
\item ovviamente $H(X)\geq 0$ perché somma di quantità positive (consideriamo gli addendi come $-\log(x)$ e ricordando che $x\in (0,1]$). Per quanto riguarda l'uguaglianza, dato che tutti gli addendi della sommatoria sono positivi, abbiamo che $H(X)=0$ se e solo se $p_j\log(p_j)=0 \  \forall j$, quindi abbiamo che $p_j$ sarà uguale ad 1 o 0, ma non può essere che tutti i $p_j$ siano  uguali a 0 e dunque deve esistere almeno un $p_j=1$.
\item  per prima cosa supponiamo che $p_j > 0$ (nel caso non lo fossero basterebbe togliere i $\  p_k=0$ e dimostrare che $H(X)\leq \log(n-c)\leq \log(n)$ dove $c$ è il numero di $p_k=0$).\\
Dalla definizione abbiamo:
\[
\begin{split}
H(x)-\log{(n)}
&=-\frac{1}{ln(2)} \bigg( \sumj p_j ln (p_j) + ln(n) \bigg)\\
&=-\frac{1}{ln(2)} \bigg( \sumj p_j (ln (p_j) + ln(n)) \bigg)\\
&=-\frac{1}{ln(2)} \bigg( \sumj p_j ln(p_jn)\bigg)\\
&=\frac{1}{ln(2)} \bigg( \sumj p_j ln \bigg( \frac{1}{p_jn} \bigg) \bigg)\\
&\leq \frac{1}{ln(2)} \bigg( \sumj p_j \bigg( \frac{1}{p_jn} -1 \bigg) \bigg)\\
&= \frac{1}{ln(2)} \bigg( \sumj \bigg( \frac{1}{n} -p_j \bigg) \bigg) \leq 0
\end{split}
\]


dove nel per passare dalla quarta alla quinta riga abbiamo usato il fatto che $ln(x)\leq x-1$ con l'uguaglianza solo se $x=1$. Quindi abbiamo che le disuguaglianze si trasformano in uguaglianze solo se $\frac{1}{p_jn}=1$ cioè se $p_j=\frac{1}{n}$ cioè se si ha distribuzione uniforme.
\end{enumerate}
\end{proof}
\vspace{15pt}


\section{Proprietà dell'entropia}
\label{sec:PropriEntropia}
\vspace{10pt}

In questa sezione indagheremo le prime proprietà dell'entropia e dimostreremo i primi risultati che getteranno le basi per le costruzioni successive.
Può essere interessante capire come si comporta l'entropia nel caso in cui le variabili in considerazione siano dipendenti, per fare ciò definiamo l'\textit{entropia condizionata}. 
\begin{defi} \label{defi:condiz}
Si dirà \textbf{entropia condizionale di $Y$ data $X=j$} la funzione: 
\begin{equation}\label{eq:6.6}
H_j(Y):=-\sum_{k=1}^m p_j(k)\log(p_j(k))
\end{equation}
\end{defi}

Prendiamo ora una variabile casuale $X$, possiamo considerare la variabile casuale $H.(Y)$ che avrà immagine $\{H_1(Y)...H_n(Y) \}$ e legge di probabilità $\{ p _1...p_n\}$. Avremo quindi che $H.(Y)$ sarà funzione di $X$.
\begin{defi}
Si dirà \textbf{entropia condizionale di $Y$ data $X$},la funzione:
\begin{equation}\label{eq:6.7}
H_X(Y):= \mathbb{E}[H.(Y)]= \sum_{j=1}^n p_j H_j(Y)
\end{equation}
\end{defi}
\begin{oss}
Più avanti, analogamente a quanto detto per la probabilità condizionata, ci sarà più comodo scrivere $H_X(Y)$ come $H(Y|X)$.
\end{oss}
\begin{lem} \label{lemma:6.8}
\begin{equation} \label{eq:6.8}
H_X(Y)=-\sum_{j=1}^n\sum_{k=1}^m p_{jk}\log(p_j(k))
\end{equation}
\end{lem}
\begin{proof}
Sostituendo \ref{eq:6.6} in \ref{eq:6.7} otteniamo

\begin{equation} \label{eq:6.8.1}
H_X(Y)=-\sum_{j=1}^n\sum_{k=1}^m p_{j}p_j(k)\log(p_j(k))
\end{equation}

Ricordando che 

$$p_j(k)=\mathbb{P}(Y=k|X=j)\ e \ p_j=\mathbb{P}(X=j)$$

otteniamo che 
$$p_jp_j(k)=\mathbb{P}(X=j)\mathbb{P}(Y=k|X=j)=\mathbb{P}(X=j,Y=k)=p_{jk}$$
sostituendo questo risultato in \ref{eq:6.8.1} possiamo concludere.
\end{proof}

\begin{lem} \label{lemmma:6.4}
se $X$ e $Y$ sono indipendenti allora vale:
\begin{equation} \label{lemma:6.4}
H_X(Y)=H(Y)
\end{equation}
\end{lem}
\begin{proof}
Sia $\{q_1...q_m\}$  la legge di probabilità di $Y$ allora ci basterà notare che nel caso in cui $X$ e $Y$ siano indipendenti $p_j(k)=\mathbb{P}(Y=k|X=j)=\mathbb{P}(Y=k)=q_k$
e dunque \ref{eq:6.8.1} diventerà
$$H_X(Y)=-\sum_{k=1}^m q_k \log(q_k)\sum_{j=1}^n p_{j}=-\sum_{k=1}^m q_k \log(q_k)1=H(Y)$$
\end{proof}

\begin{defi} \label{defi:congiun}
Siano $X$ e $Y$ due variabili casuali definite sullo stesso spazio di probabilità, definiamo la loro \textbf{entropia congiunta} $H(X,Y)$ come:
\begin{equation}\label{eq:congiun}
H(X,Y):=-\sum_{j=1}^n\sum_{k=1}^m p_{jk} \log(p_{jk})
\end{equation}
dove con $p_{jk}$ intendiamo \p$(X=j,Y=k)$
\end{defi}
\begin{oss}
Dalla definizione si ha immediatamente che $H(X,Y)=H(Y,X)$.
\end{oss}
\begin{teo} \label{teo:6.5}
Date due variabili casuali $X,Y$ vale:
\begin{equation}
H(X,Y)=H(X)+H_X(y).
\end{equation}
\end{teo}
\begin{proof}
sapendo che \p$(A \cap B)=\mathbb{P}(A|B)$\p$(B)$ e quindi che $p_{jk}=p_jp_j(k)$ possiamo sostituire direttamente nella definizione di entropia congiunta \ref{defi:congiun} ottenendo:
$$H(X,Y)=-\sum_{j=1}^n\sum_{k=1}^m p_{jk} \log(p_{j}p_{j}(k))=-\sum_{j=1}^n\sum_{k=1}^m p_{jk} \log(p_{j}(k))-\sum_{j=1}^n\sum_{k=1}^m p_{jk} \log(p_{j})$$
possiamo concludere ricordando che $\sum_{k=1}^m p_{jk}=p_j$
\end{proof}
\begin{corol}
se $X$ e $Y$ sono indipendenti allora vale:
\begin{equation}
H(X,Y)=H(X)+H(Y)
\end{equation}
\end{corol}
\begin{proof}
basta applicare il lemma \ref{lemmma:6.4} al teorema precedente
\end{proof}
\begin{teo} \label{teo:disugShannon}
(\textit{Disuguaglianza fondamentale di Shannon})\\
\begin{equation}
H_X(Y)\leq H(Y)
\end{equation}
\end{teo}
\begin{proof}
Per la dimostrazione utilizziamo la disuguaglianza di Jensen:\\
data $f$ funzione convessa vale
\begin{equation}
\sumj \lambda_j f(x_j)\geq f\bigg( \sumj \lambda_j x_j \bigg)
\end{equation}
con $\lambda_j > 0$ e $\sumj \lambda_j =1$
per la dimostrazione si veda \cite{Jensen}.\\
Ora applicando la disuguaglianza con:
$$\lambda_j=p_j,\ f(x)=x \log { (x)}, \  x_j=p_j(k)$$
per k fissato, otteniamo quindi:
$$\sumj p_j p_j(k)\log {(p_j(k))}\geq \sumj \bigg( p_j p_j(k) \bigg) \log { \bigg( \sumj p_j p_j(k) \bigg) }=q_k \log {(q_k)}$$
dove l'uguaglianza la ricaviamo da: $\sumj p_j p_j(k)=\sumj \bigg( \mathbb{P}(X=j) \mathbb{P}(Y=k|X=j) \bigg)= \mathbb{P}(Y=k)= q_k$.
Sommando su $k$ abbiamo che la parte sinistra della disuguaglianza diventa:
$$\sumj  p_j \sumk p_j(k) \log {(p_j(k))}=-\sumj p_j H_k(Y)=-H_X(Y)$$
mentre a destra otteniamo
$$\sumk q_k \log { (q_k) }= -H(Y)$$
e quindi:
\begin{equation}
-H_X(Y) \geq -H(Y)
\end{equation}
Da cui possiamo concludere direttamente.
\end{proof}
Questo risultato può essere pensato come: aggiungendo informazione (il valore di $X$) l'entropia del sistema diminuisce.
\begin{oss}\label{oss:disugShannon}
Nel caso di \textit{processi stocastici} (si veda \ref{sec:markEntropia} per la definizione) è comodo osservare che considerando $Y=(X_{n+1})$ e $X=X_0$ nel teorema precedente si ottiene:
$$H(X_{n+1}|X_0,X_1...X_n) \leq H(X_{n+1}|X_1...X_n).$$
\end{oss}
\vspace{15pt}


\section{Unicità dell'Entropia}
\label{sec:UniEntropia}
\vspace{10pt}

Si può dimostrare che la scelta della funzione di entropia come \textit{misura di incertezza} è unica a meno di una costante moltiplicativa.\\
Ptrima di definire la \textit{misura di incertezza} premettiamo una precisazione sulla notazione.\\
Indicheremo la probabilità condizionata ($\mathbb{P}(Y=k|X=j)$) con la notazione $p_{j}(k)$ oppure, in modo totalmente equivalente, $p(k|j)$.
\begin{defi} \label{defi:misuraincertezza}
sia \spacep un spazio di probabilità e $X$ variabile casuale discreta di legge $\{ p_1....p_n \}$ , una funzione $U$ viene detta \textbf{misura di incertezza} se soddisfa le seguenti condizioni:
\begin{enumerate}
\item $U(X)$ è un massimo quando ha distribuzione uniforme
\item $U(p_1...p_n,0)=U(p_1...p_n)$
\item $U(p_1....p_n)$ è continua per tutti i suoi argomenti.
\item presa $Y$ variabile casuale allora $U(X,Y)=U_X(Y)+U(X)$\\
 dove $U_X(Y)= \sumj p_j U(Y|X=j)$ ricordando che $(Y|X=j)$ può essere vista come una variabile casuale di legge di probabilità  $\{ p_j(1)...p_j(m) \}$ 
\end{enumerate}
\end{defi}

\begin{teo} \label{teo:misuraIncertezza}
In uno spazio di probabilità \spacep consideriamo una variabile casuale $X$ con \lep allora\\
$U(X)$ è una misura di incertezza se e solo se 
$$U(X)= KH(X)$$
dove $K$ è una costante $K\geq 0$
\end{teo}
Per la dimostrazione si veda \cite{Khinchin} pag.10.


\begin{defi} \label{defin:mutua}
date \var definiamo \textbf{mutua informazione di $X$ e $Y$}
\begin{equation} \label{defi:mutua}
I(X,Y):=H(Y)-H_X(Y)
\end{equation}
\end{defi}

Notiamo che $H_X(Y)$ è l'informazione contenuta in $Y$ che non è contenuta in $X$ e quindi l'informazione di $Y$ contenuta in $X$ sarà $H(Y)-H_X(Y)=I(X,Y)$
\begin{teo} \label{teo:6.7}
Siano $X$ e $Y$ due variabili casuali rispettivamente  \lep e  $\{q_1...q_n \}$
\begin{enumerate}
\item $I(X,Y)=\sumj \sumk p_{jk} \log \bigg( \frac{p_{jk}}{p_j q_k} \bigg)$
\item $I(X,Y)=I(Y,X)$
\item se $X$ e $Y$ sono indipendenti allora $I(X,Y)=0$
\end{enumerate}
\end{teo}
\begin{proof}
si proceda come segue:
\begin{enumerate}
\item sempre ricordando che $\sum_{k=1}^m p_{jk}=p_j$ possiamo scrivere 
$$H(Y)=-\sumk q_{k} \log (q_k)=-\sumj \sumk p_{jk} \log (q_k)$$
e dunque per \ref{eq:6.8} otteniamo
$$I(X,Y)=-\sumj \sumk p_{jk} \log(q_k)+\sumj \sumk p_{jk}\log{(p_j(k))}$$
\item immediato da 1.
\item semplicemente ricordando che se $X$ e $Y$ sono indipendenti $H_X(Y)=H(Y)$
\end{enumerate}
\end{proof}

\vspace{15pt}


\section{Principio dell'Entropia Massima}
\label{sec:maxEntropia}
\vspace{10pt}

Spesso ci si trova in condizioni in cui è data una variabile casuale $X$ a valori $\{ x_1...x_n \}$ di cui non si conosce la \lep  in questi casi si può applicare il principio di entropia massima:\\
\begin{defi}
Data una una variabile casuale $X$ con \lep  incognita il \textbf{principio dell'entropia massima} ci impone di scegliere i $p_j$ in modo tale che $H(X)$ sia massima
\end{defi}
\textbf{Esempio.} 
Sia $X$ una variabile casuale a valori $\{ x_1...x_n \}$ di cui non si conosce la \lep . Sappiamo già che, se non ci sono altre condizioni, l'entropia sarà massima se $X$ sarà uniformemente distribuita. Prendiamo ora il caso in cui ci venga fornita la media di $\mathbb{E}[ X]=E$. Per trovare il massimo dell'entropia $H(X)$ utilizziamo il metodo dei  moltiplicatori di Lagrange:\\
come costrizioni abbiamo:
\begin{enumerate}
\item $\sumj p_j=1$
\item $\sumj x_j p_j=E$
\end{enumerate}
Dunque dobbiamo trovare il massimo valore di:
\begin{equation}
L(p_1...p_n;\lambda,\mu ):= -\sumj p_j \log(p_j) + \lambda \bigg( \sumj p_j -1 \bigg) + \mu \bigg( \sumj x_j p_j - E \bigg)
\end{equation}
dove $\lambda$, $\mu$ sono i moltiplicatori di Lagrange.\\
Imponendo le derivate parziali uguali a 0 otteniamo:
$$\frac{\partial L}{\partial p_j}=-\frac{1}{ln(2)}(ln(p_j)+1)+\lambda + \mu x_j=0 \ \  (1\leq j \leq n) $$
$$quindi$$
$$p_j=e^{\lambda ' + \mu ' x_j} \ \ (1\leq j \leq n)$$
dove $\lambda ' = ln(2) \lambda -1 $ e $\mu ' = ln(2) \mu$\\
da 1. possiamo ricavare l'equazione $0=\sumj p_j -1= \sumj e^{\lambda ' + \mu ' x_j} -1$ che risolta ci restituisce:
$$\lambda ' = - ln(Z( \mu '))$$
dove $Z( \mu '):= \sumj e^{\mu ' x_j}$.\\
questo ci permette di riscrivere $p_j$ come:
\[
\begin{split}
p_j &= e^{\lambda ' + \mu ' x_j} \\
& = e^{- ln(Z( \mu ')) + \mu ' x_j}  \\
& = \frac{e^{\mu ' x_j}}{Z( \mu ')}
\end{split}
\]
Per $\mu'$ invece possiamo usare 2. ottenendo 
\[
\begin{split}
E &=\sumj x_j p_j \\
& = \sumj x_j \frac{e^{\mu ' x_j}}{Z( \mu ')}
\end{split}
\]
riassumendo quindi abbiamo:
\begin{equation}
p_j=\frac{e^{\mu ' x_j}}{Z( \mu ')} \ \ (1\leq j \leq n)
\end{equation}
Dove $\mu'$ dipenderà dalla distribuzione $x_i$ e verrà calcolato caso per caso

\vspace{15pt}


\section{Entropia nelle catene di Markov}
\label{sec:markEntropia}
\vspace{10pt}

\begin{defi}
Si consideri una famiglia di variabili casuali tutte definite sullo stesso spazio di probabilità \spacep , $(X(t), t \geq 0 )$, tale famiglia è detta \textbf{processo stocastico}.
\end{defi}
Nella nostra trattazione ci limiteremo a considerare una piccola classe di processi stocastici chiamati catene di Markov.
\begin{defi} \label{defi:cateMarkov}
Un processo stocastico è detto \textbf{catena di Markov} se 
\begin{enumerate}
\item l'insieme $S$ che comprende i valori ammissibili delle variabili $X_n$ è discreto (se $S$ è denso si dirà processo di Markov)
\item possiede la 'proprietà di Markov' cioè
 $$\mathbb{P}(X_{n+1}=k_{n+1}|X_n=k_n,.,X_0=k_0)=\mathbb{P}(X_{n+1}=k_{n+1} | X_n=k_n)$$
\end{enumerate}
\end{defi}
Per i nostri scopi inoltre considereremo l'insieme del tempo come come un insieme discreto contenuto in $\mathbb{N}$.\\
Infine, definita $p_{ij}^n:=\mathbb{P}(X_{n+1}=j|X_n=i)$ vogliamo che la nostra matrice di transizione $P$, formata dai vari $p_{ij}$, sia stazionaria cioè:\\
$$p_{ij}^n=p_{ij}^0=: p_{ij} \  \forall n$$
Ci domandiamo ora se è sempre possibile definire una catena di Markov $X=(X_n,n\in\mathbb{Z})$ per la quale ogni $X_n$ ammette entropia massima cioè per il teorema \ref{teo:6.2} $X_n$ ha distribuzione uniforme (se non vi sono altre restrizioni).
Premettiamo alcune definizioni
\begin{defi}
Sia $X=(X_n, n \in \mathbb{N})$ una catena di Merkov con matrice di transizione $P$. Un vettore di probabilità $\rho$ è detto \textbf{distribuzione invariante o stazionaria per X} se:
$$\rho= \rho P$$
\end{defi}

\begin{defi}
Una matrice A ad elementi positivi è detta \textbf{bistocastica} se per ogni riga e per ogni colonna la somma dei suoi elementi è pari ad 1.
\end{defi}
\begin{teo}
Una catena di Markov con matrice di transizione $P$ ammette la distribuzione uniforme come distribuzione invariante se e solo se $P$ è bistocastica
\end{teo}
\begin{proof}
Se $P$ è bistocastica allora $ \sumi P_{ij}=1 \forall 1\leq j \leq N$ e quindi:
$$\sumi \frac{1}{N}P_{ij}=\frac{1}{N}\sumi P_{ij}=\frac{1}{N}$$
e quindi $\frac{1}{N}$ è una distribuzione invariante.\\
Supponiamo ora che la distribuzione uniforme sia invariante e dimostriamo che $P$ è bistocastica. Essendo $P$ una matrice di transizione abbiamo già che la somma degli elementi di una riga sarà 1. Dimostriamo che anche la somma degli elementi di una colonna è pari a 1. Procedendo al contrario di prima abbiamo:
$$\frac{1}{N}=\frac{1}{N} \sumi P_{ij} \Rightarrow \sumi P_{ij}=1$$
Abbiamo che per ogni colonna la somma dei suoi elementi è 1, quindi possiamo concludere
\end{proof}

\begin{defi}
Siano $X$ e $Y$ due variabili casuali della stessa dimensione con legge di probabilità rispettivamente $\{p_1...p_n \}$ e $\{q_1...q_n \}$.
Definiamo \textbf{entropia relativa} il valore:
\begin{equation}
D(X,Y):=\sumj p_j \log \bigg( \frac{p_j}{q_j} \bigg)
\end{equation}
\end{defi}
\begin{teo}
\begin{enumerate}
Per l'entropia relativa vale:
\item $D(X,Y)\geq 0, =$ se e solo se $X$ e $Y$ sono identicamente distribuite
\item se $Y$ è uniformemente distribuita allora vale
\begin{equation}
D(X,Y)= \log(n)-H(X)
\end{equation}
\end{enumerate}
\end{teo}
\begin{proof}\leavevmode 
\begin{enumerate}
\item riscriviamo il primo punto come:
\[
\begin{split}
D(X,Y) & =\sumj p_j \log \bigg( \frac{p_j}{q_j} \bigg)\\
& =\sumj p_j \log (p_j) -\sumj p_j \log (q_j)
\end{split}
\]

E da qui si può concludere applicando la disuguaglianza di Gibbs: $- \sumin p_i \log(p_i) \leq - \sumin p_i \log(q_i)$ per $\{p_1...p_n \}$ e $\{q_1...q_n \}$ distribuzioni di probabilità (deriva immediatamente dal caso continuo \ref{teo:GibbsContinuo})
\item essendo $Y$ uniformemente distribuita avremo che $q_j=1\frac{1}{n}$ e quindi dalla definizione di entropia relativa abbiamo:
\[
\begin{split}
D(X,Y) & =\sumj p_j \log \bigg( \frac{p_j}{q_j} \bigg) \\
& =\sumj p_j \log ( p_j n) \\
& =\sumj p_j \log (p_j) + \sumj p_j \log (n) \\
& =-H(X)+ \log(n) \sumj p_j= \log(n)-H(X)
\end{split}
\]

\end{enumerate}
\end{proof}
\begin{defi}
Un processo stocastico è detto \textbf{stazionario} se presi $m,k\in \mathbb{N}$ vale:
$$\mathbb{P}(X_{n_1}=i_1...X_{n_k}=i_k)= \mathbb{P}(X_{n_1+m}=i_1...X_{n_k+m}=i_k) \ \forall i_s \in S$$
\end{defi}
Consideriamo una catena di Markov stazionaria $(X_n,n \in \mathbb{Z}_+)$ con una matrice di transizione bistocastica $P$ e sia $X_{\infty}$ una variabile casuale di dimensione $n$ uniformemente distribuita, abbiamo quindi che $D(X_n,X_{\infty})=\log(N)-H(X_n)$. Si dimostra che $D(X_n,X_{\infty})$ è una funzione decrescente e che se la distribuzione uniforme è l'unica distribuzione invariante, allora $\lim_{n \to \infty}D(X_n,X_{\infty})=0$. Segue quindi che $H(X_n)$ è crescente. \cite{Thomas}


\vspace{15pt}


\section{La Regola della Catena}
\label{sec:chainRule}
\vspace{10pt}

Vediamo ora come cambia l'informazione in un processo stocastico. L'approccio più naturale può sembrare quello di considerare l'entropia come funzione del tempo,come si è cominciato a fare sopra, in questo modo però ci si dimentica della relazione che esiste tra due passaggi successivi, dal tempo $t_n$ al tempo $t_{n+1}$. Procederemo quindi in modo differente cominciando dal generalizzare i risultati visti nel caso di due sole variabili.\\
Estendiamo la definizione di entropia congiunta ( \ref{defi:congiun} ) in questo modo:
\begin{equation}
H(X_0...X_n):=- \suma p(i_0...i_n) \log(p(i_o...i_n)).
\end{equation}
Mentre la definizione di entropia condizionata ( \ref{defi:condiz} ) nel caso multivariato sarà:
\begin{equation}
H(Y|X_1...X_n)=-\sum_{j,i_1...i_n=1}^N \mathbb{P}(Y=j,X_1=i_1...X_n=i_n) \log(\mathbb{P}(Y=j|X_1=i_1...X_n=i_n))
\end{equation}
Non ci rimane che generalizzare il teorema \ref{teo:6.5}.
\begin{teo} \label{teo:chainRule}
\textbf{Regola della catena}
\begin{equation}
H(X_0...X_n)=H(X_0)+\sum_{i=1}^n  H(X_i|X_0,...X_{i-1})=H(X_0)+H(X_1|X_0)+...+H(X_n|X_0...X_{n-1})
\end{equation}
Inoltre l'entropia congiunta cresce al crescere di n.
\end{teo}
\begin{proof}
Procediamo per induzione:\\
Il caso base con $n=1$ è esattamente il teorema \ref{teo:6.5}, procediamo con il passo induttivo. Quindi assumiamo che valga per $n$, dimostriamo che vale per $n+1$.
$$H(X_0...X_n, X_{n+1})=- \sumaa p(i_0...i_n,i_{n+1}) \log(p(i_o...i_n,i_{n+1}))$$
$$= - \sumaa p(i_0...i_n,i_{n+1}) \log(\mathbb{P}(X_{n+1}=i_{n+1}|X_0=i_0,...X_n=i_n))-\sumaa p(i_0...i_n,i_{n+1}) \log(p(i_0...i_n))$$
dato che 
$$\sumaa p(i_0...i_{n+1}) \log(p(i_0...i_n))=\suma p(i_0...i_{n}) \log(p(i_0..i_n))$$
abbiamo che
\begin{equation}\label{eq:10.8}
H(X_0...X_{n+1})=H(X_0...X_n)+H(X_{n+1}|X_0...X_n)
\end{equation}
Applicando l'ipotesi induttiva otteniamo il risultato.
Inoltre da \ref{eq:10.8} e dal fatto che l'entropia condizionata è sempre maggiore di zero otteniamo che l'entropia congiunta cresce nel tempo.
\end{proof}

\vspace{15pt}


\section{Velocità dell'Entropia}
\label{sec:EntropyRate}
\vspace{10pt}

\begin{defi}
Quando il limite esiste, $h(X)$ si dice \textbf{velocità dell'entropia} dove
$$h(X):=\limi\frac{1}{n}H(X_0...X_{n-1})$$
\end{defi}

\begin{teo}\label{teo:10.10}
se $X=(X_i,i\in \mathbb{N})$ è un processo stocastico stazionario, allora $h(X)$ esiste e:
\begin{equation} \label{eq3}
h(X)=\limi H(X_{n-1}|X_0...X_{n-2})
\end{equation}
\end{teo}
\begin{proof}
Applicando la regola della catena \ref{teo:chainRule} otteniamo subito che
\begin{equation}\label{eq4}
h(X)=\limi \frac{1}{n} H(X_0...X_{n-1})= \limi \frac{1}{n} \sum_{i=0}^{n-1}H(X_i|X_0...X_{i-1})
\end{equation}
Passiamo ora a dimostrare l'esistenza del secondo membro di \ref{eq3}:\\
dall'osservazione \ref{oss:disugShannon} otteniamo:
\begin{equation} \label{eq1}
H(X_{n+1}|X_0,X_1...X_n) \leq H(X_{n+1}|X_1...X_n)
\end{equation}
Grazie al Teorema \ref{teo:6.5} possiamo scrivere:
$$H(X_{n+1}|X_1...X_n)=H(X_{n+1},X_1...X_n) - H(X_1....X_n)$$
e ricordandoci che il processo è stazionario abbiamo:
$$H(X_{n+1},X_1...X_n) - H(X_1....X_n)= H(X_{n},X_0...X_{n-1}) - H(X_0....X_{n-1})$$
infine applicando il Teorema \ref{teo:6.5} in modo inverso rispetto a prima
$$H(X_{n},X_0...X_{n-1}) - H(X_0....X_{n-1})=H(X_{n}|X_0...X_{n-1})$$
riassumendo quindi
\begin{equation} \label{eq2}
H(X_{n+1}|X_1...X_n)=H(X_{n}|X_0...X_{n-1})
\end{equation}
Sostituendo \ref{eq2} in \ref{eq1} otteniamo:
\begin{equation}
H(X_{n+1}|X_0,X_1...X_n) \leq H(X_{n}|X_0...X_{n-1})
\end{equation}
Quindi definendo $a_n :=  H(X_{n}|X_0...X_{n-1})$ otteniamo una successione $\{ a_n \}_{n\in \mathbb{N}}$ monotona non crescente limitata dal basso visto che $a_k=H(X_{k}|X_0...X_{k-1})\geq 0$ e dunque $\limi a_n$ esiste ed è finito dato che $H(Y)< \infty $.
Controlliamo ora che  $ \lim_{n \to \infty} \frac{1}{n} \sumin a_i$ converga ad $a$ dove $a:=\limi a_n$:
$$\limi \bigg| \frac{1}{n} \sumin a_i-a \bigg| \leq \limi \frac{1}{n} \sumin |a_i-a| = \limi \frac{1}{n} \bigg( \sum_{i=1}^{N_0} |a_i-a| + \sum_{i=N_0+1}^{n} |a_i-a| \bigg) = \limi \sum_{i=N_0+1}^{n} \frac{|a_i-a|}{n}$$
Come ci si aspettava la successione converge, possiamo notare che tale limite altro non era che il limite delle medie di \textit{Cesaro}.
E da qui possiamo concludere scegliendo $N_0$ tale che $\frac{1}{n}|a_i-a|$ sia piccolo a piacere, cosa sempre possibile dato che $\limi a_n=a$.\\
Ricordando come abbiamo definito $a_n$ otteniamo quindi:
\begin{equation}
\limi H(X_{n-1}|X_0...X_{n-2})=\limi \frac{1}{n} \sum_{i=0}^{n-1}H(X_i|X_0...X_{i-1})
\end{equation}
ricordando infine \ref{eq4} possiamo concludere:
$$h(X)=\limi \frac{1}{n} H(X_0...X_{n-1}) = \limi H(X_{n-1}|X_0...X_{n-2}).$$
\end{proof}
\begin{teo}
Se $(X_i\in \mathbb{N})$ è una catena di Markov stazionaria con distribuzione iniziale $\pi^{(0)}$ e matrice di transizione $P$ allora vale
\begin{equation}
h(X)=\sumij \pi^{(0)} P_{ij} \log {(P_{ij})}
\end{equation}
\end{teo}
\begin{proof}
dal teorema precedente \ref{teo:10.10} abbiamo:
\[
\begin{split}
h(X) &=\limi H(X_{n-1}|X_0...X_{n-2})\\
&=\limi H(X_{n-1}|X_{n-2})\\
&=H(X_1|X_0)\\
&=\sumij \mathbb{P}(X_0=i,X_1=j)\log {(P_{ij})}\\
&=\sumij \mathbb{P}(X_0=i)P_{ij}\log {(P_{ij})}\\
&=\sumij \pi^{(0)}P_{ij}\log {(P_{ij})}
\end{split}
\]



Dove per passare dalla prima alla seconda riga abbiamo usato la 'proprietà di Markov' \ref{defi:cateMarkov}, per passare dalla seconda alla terza abbiamo usato il fatto che il processo è stazionario, dalla terza alla quarta il lemma \ref{lemma:6.8}
\end{proof}





