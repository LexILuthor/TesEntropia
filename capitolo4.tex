\chapter{Conclusioni}
\label{cha:conclusioni}
\vspace{15pt}

Per concludere, in questo elaborato va evidenziato l'aspetto peculiare di alcuni dei risultati ottenuti, che concordano pienamente con l'idea intuitiva che si può esigere dalla parola \textit{entropia}, altri invece, come il teorema Fondamentale di Shannon, richiedono uno studio più approfondito per essere compresi nella loro interezza.\\
A titolo d'esempio si considerino due disuguaglianze: $H(X)\leq(n)$ \ref{teo:6.2} e la \textit{disuguaglianza di Shannon} \ref{teo:disugShannon} ($H_X(Y)\leq H(Y)$). La prima delle due mostra come aumentando $n$ ovvero aumentando i possibili risultati di $X$ l'andamento  del sistema diventa più imprevedibile ed infatti il tetto massimo dell'entropia, $\log(n)$, continua a crescere. La \textit{disuguaglianza di Shannon} invece descrive come si comporta l'entropia di una variabile casuale $Y$ nel caso in cui vengano fornite nuove informazioni e quindi nel caso in cui il sistema diventi più prevedibile.\\ 
Un commento particolare lo merita l'ultimo teorema dimostrato. Infatti, se ci si concentra solo sulla riduzione dell'errore commesso, si può essere portati a sottovalutare la portata del teorema di Shannon. Difatti, per ridurre l'inesattezza si potrebbe semplicemente pensare di inviare più volte il simbolo che si vuole trasmettere. In questo modo, indicando con $p<\frac{1}{2}$ la probabilità d'errore commesso inviando un simbolo, avremo che affinché il sistema registri un errore nella ricezione servirebbe un errore su almeno $\frac{n}{2}$ simboli. Questo evento verrebbe modellizzato attraverso una variabile casuale binomiale che, al crescere di $n$, farebbe tendere la probabilità di tale evento a zero. Questo procedimento però, all'aumentare di $n$ ridurrebbe anche la velocità di trasmissione d'informazione mandandola a zero. La forza del \textit{teorema fondamentale di Shannon} sta proprio in quest'osservazione. Il teorema infatti garantisce l'esistenza di un codice che, mandando a zero l'errore commesso, mantiene comunque la velocità di trasmissione di informazione arbitrariamente vicina alla capacità del canale. Purtroppo però questo teorema non è di tipo costruttivo; non fornisce cioè un metodo per la creazione di tale codice lasciandone quindi ancora aperta la ricerca.\\
È stato dimostrato che l'inverso non è possibile, cioè non si può avere una probabilità d'errore arbitrariamente piccola se si trasmette ad una capacità superiore a quella del canale.\\
Come accennato nella discussione del teorema ricordiamo che  è stata dimostrata la possibilità non solo di poter controllare la media di errore, ma anche quella massima ($\max_{1\leq i \leq M} \mathbb{P}(E|x_i)$) generando così un maggior controllo sul canale.







